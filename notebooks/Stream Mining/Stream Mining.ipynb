{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae95b85",
   "metadata": {},
   "source": [
    "# Stream Mining\n",
    "\n",
    "Stream mining is the practice of data mining a continuous stream of data records. This data stream either arrives too fast or the complete stream is too large such that it is not feasible to process using conventional data mining techniques, even ones that are tailored for big data.\n",
    "\n",
    "One common aspect of data mining streams is that it has to be processed quickly, usually as soon as the data arrives. We do not have the luxury of first storing and then analyzing the data at a later time that is more convenient. In this notebook we will show the techniques for doing the following analysis on streaming data:\n",
    "\n",
    "1. **Sampling**\n",
    "2. **Filtering**\n",
    "3. **Counting Distinct Elements**\n",
    "4. **Estimating Moments**\n",
    "5. **Counting Ones**\n",
    "6. **Most Commong \"Recent\" Elements**\n",
    "\n",
    "Most of the algorithms for stream mining involve summarizing the stream in some way. These techniques utilizes hash functions in their implementation. In most cases, it will be better to just get the \"approximate\" answer bounded by some error, instead of getting the exact answer which may not be feasible to compute.\n",
    "\n",
    "## Stream Data Model\n",
    "\n",
    "<img src=\"StreamDataModel.png\" height=600 width=400>\n",
    "\n",
    "We show a canonical example of a streaming data management system above. A number of stream is feeeing into the system, each providing data on its own schedule. The data from the stream need be uniform both in the data type and the rate at which the data arrives. The fact that the rate of arrival of stream elements is not under the control of the system is the main distinguishing factor between stream mining and a regular data mining system.\n",
    "\n",
    "Streams may eventually be saved in a longer term archival storage like a disk or database, but we have to assume that we cannot answer analytics queries using the archived data because retrieving it is a time-consuming process and the latency of the streaming data coming in is much faster. Instead there is a limited working storage which can be accessed faster that we can use, but it has insufficient capacity to store the entire stream.\n",
    "\n",
    "### Stream Sources\n",
    "\n",
    "1. Sensor Data\n",
    "2. Video Cameras\n",
    "3. Internet Web Traffic\n",
    "4. Server Logs\n",
    "\n",
    "### Stream Queries\n",
    "\n",
    "There are two main types of queries we want to make of streaming data. The first are standing queries which are permanently executing. For example if we have streaming data from temperature sensors, our standing queries could be:\n",
    "\n",
    "* Output alerts when temperature data passes a certain threshold.\n",
    "* Average temperature using the last $n$ elements in a stream.\n",
    "* What's the maximum temperature ever recorded in the stream\n",
    "\n",
    "All of these are calculations that can be done quickly when new data appears in the stream with no store the stream in its entirety. \n",
    "\n",
    "The other form of queries are *adhoc queries*. Here the queries can be one-time or the parameter of the query can be changing based on the need. A common approach would be to store a sliding window of the $n$ most recent data in the working storage of the stream management system. \n",
    "\n",
    "### Examples of Stream Processing Systems\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"kafka.png\" height=180 width=180></td>\n",
    "        <td><img src=\"kinesis.png\" height=200 width=200></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Two examples of streaming systems are: Apache Kafka (https://kafka.apache.org) and Amazon Kinesis (https://aws.amazon.com/kinesis/). For the examples in this notebook we will Apache Kafka to simulate data coming from a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7afaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "random.seed(10) #Set seed for repeatability\n",
    "\n",
    "#Hash a string to an integer\n",
    "def hashInt(val, buckets=sys.maxsize, hashType='default'):\n",
    "    if hashType == 'default':\n",
    "        return hash(val) % buckets\n",
    "    elif hashType == 'md5':\n",
    "        return hash(hashlib.md5(val.encode('utf-8')).hexdigest()) % buckets\n",
    "\n",
    "#Hash a string to a bit string\n",
    "def hashBits(val, hashType='default'):\n",
    "    if hashType == 'default':\n",
    "        return bin(hash(val))\n",
    "    elif hashType == 'md5':\n",
    "        return bin(hash(hashlib.md5(val.encode('utf-8')).hexdigest()))\n",
    "    \n",
    "words = np.array(open('words.txt').read().splitlines())\n",
    "userIds = np.arange(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e115e",
   "metadata": {},
   "source": [
    "### Kafka Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14549474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafka\n",
    "\n",
    "#Build our streams\n",
    "words = np.array(open('words.txt').read().splitlines()) #copied from /usr/share/dict/words\n",
    "userIds = np.arange(1, 1000)\n",
    "\n",
    "producer = kafka.KafkaProducer()\n",
    "\n",
    "#Create streams of 2 million elements\n",
    "for i in range(1000000):\n",
    "    if (i+1) % 100000 == 0:\n",
    "        print(i+1)\n",
    "        \n",
    "    producer.send('sampling', (str(np.random.choice(words))).encode('utf-8'))\n",
    "\n",
    "    producer.flush()\n",
    "    \n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = kafka.KafkaConsumer('test', auto_offset_reset='earliest')\n",
    "n = 0\n",
    "for msg in consumer:\n",
    "    print(msg.value.decode(\"utf-8\"))\n",
    "    n += 1\n",
    "    if n == 8:\n",
    "        break\n",
    "        \n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb88754",
   "metadata": {},
   "source": [
    "## Sampling Data in a Stream\n",
    "\n",
    "We start with the general problem of sampling data. We want to be able to select a subset of the stream to make calculations easier, but also have the results be statistically representative of the stream as a whole. One example might be a search engine wanting to answer the question of \"What fraction of the typical userâ€™s queries were repeated over the past month?\". Assume also that due to space constraints, we wish to store only $\\frac{1}{10}$th of stream elements.\n",
    "\n",
    "An easy but incorrect approach would be to just generate a random number from 0 to 9 for each incoming data, and only consider data when the random number is equal to 0. However this will give us the wrong answer as for each user we would be drastically reducing the fraction of repeated queries too. If a user has $s$ queries which were done exactly once, and $d$ queries which were done exactly twice, and no queries were done more than 2 times. The correct answer for the fraction of repeated queries for this user is $\\frac{d}{s+d}$. However using the sampling method just detailed, the fraction we will get is $\\frac{d}{10s + 19d}$.\n",
    "\n",
    "The right way would be to only consider $\\frac{1}{10}$th of all users, but consider all queries for those randomly selected users. Assume further that the possible list of users is so large that isn't feasible to store which users should have their queries tracked and which users shouldn't. We can instead use a hash function that maps the user to 1 of 10 buckets. If the user hashes to the first bucket, then this user is someone whose queries we would want to track. Note that we do not actually need to save the users into the buckets, we just need to know at runtime which bucket the user goes to using the fast hash function. This saves us from having to keep track of all users in the universal set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd40469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 100000 Average Repeat Per User: 8.48824378236143e-05\n",
      "Iterations: 200000 Average Repeat Per User: 0.0004026243221023727\n",
      "Iterations: 300000 Average Repeat Per User: 0.0003966167804915212\n",
      "Iterations: 400000 Average Repeat Per User: 0.0004724339706863245\n",
      "Iterations: 500000 Average Repeat Per User: 0.00086515906940257\n",
      "Iterations: 600000 Average Repeat Per User: 0.0010044780481139246\n",
      "Iterations: 700000 Average Repeat Per User: 0.0012795931598440701\n",
      "Iterations: 800000 Average Repeat Per User: 0.0015721791194151934\n",
      "Iterations: 900000 Average Repeat Per User: 0.0018303686818008796\n",
      "Iterations: 1000000 Average Repeat Per User: 0.002072822397857278\n"
     ]
    }
   ],
   "source": [
    "samples = {} \n",
    "\n",
    "n = 0\n",
    "while n < 1000000:\n",
    "    userId = np.random.choice(userIds)\n",
    "    word = np.random.choice(words)\n",
    "    \n",
    "    #hash to 10 buckets. Process only if hash points to first bucket\n",
    "    if hashInt(userId, 10) == 0:\n",
    "        if userId not in samples: \n",
    "            samples[userId] = {}\n",
    "        \n",
    "        if word not in samples[userId]:\n",
    "            samples[userId][word] = 0\n",
    "        \n",
    "        samples[userId][word] += 1\n",
    "        \n",
    "    #Print fraction of repeated queries every 100000 elements\n",
    "    n += 1\n",
    "    if n % 100000 == 0:\n",
    "        fractionRepeats = 0\n",
    "        for userId in samples:\n",
    "            repeats = 0\n",
    "            \n",
    "            for word in samples[userId]:\n",
    "                if samples[userId][word] > 1:\n",
    "                    repeats += 1\n",
    "    \n",
    "            fractionRepeats += repeats / len(samples[userId])\n",
    "\n",
    "        fractionRepeats /= len(samples)\n",
    "        print(\"Iterations:\", n, 'Average Repeat Per User:', fractionRepeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f700aaf",
   "metadata": {},
   "source": [
    "### Fixed Size Samples\n",
    "\n",
    "For infinite or very large streams, our set of samples will also eventually become too large for our working storage. For cases like this we may want to fix a maximum number of samples we will keep, and randomly replace older samples with newer ones over time. One technique to do this will be to do **Reservoir Sampling**.\n",
    "\n",
    "Let $s$ be the maximum number of samples we want to keep, and $n$ be the number of samples we have been tracking from the start. \n",
    "\n",
    "1. While $n <= s$, keep sampling normally.\n",
    "2. A new element we want to samples arrives such that $n > s$. With probability $\\frac{s}{n}$, keep the new sample. Otherwise discard it.\n",
    "3. If we're keeping the new sample, discard one of the older samples at random.\n",
    "\n",
    "After $n$ elements, the sample contains each element seen so far with probability $\\frac{s}{n}$. For our user query sample above, it may still be better to think of replacing users instead of their individual queries.\n",
    "\n",
    "We implement the fixed size sampling algorithm detailed above for $s=20$ while going through our *words* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc60a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 20\n",
      "['wrencher', 'wonderfulness', 'Mantuan', 'dimorphic', 'disturbance', 'Sparassis', 'unstow', 'peastone', 'downline', 'manneristically', 'spelterman', 'darned', 'calathiform', 'oblongly', 'excavationist', 'oneirocrit', 'subloreal', 'unspiritedly', 'binomially', 'thioarsenic']\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "maxSamples = 20 #s\n",
    "samplesTracked = 0 #n\n",
    "\n",
    "for w in words:\n",
    "    samplesTracked += 1\n",
    "    \n",
    "    if len(samples) < maxSamples:\n",
    "        samples.append(w)\n",
    "    else:\n",
    "        if random.random() < (maxSamples / samplesTracked):\n",
    "            randIndex = random.randint(0, maxSamples-1)\n",
    "            samples[randIndex] = w\n",
    "\n",
    "print(\"Samples:\", len(samples))\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b76831",
   "metadata": {},
   "source": [
    "## Filtering Streams\n",
    "\n",
    "Another common use case is filtering data in a stream. In this case we have a set $S$ of data that we want to filter for, but the size of $S$ is too large to fit in memory. Additionally, it is assumed that the elements in $S$ are only a small fraction of the universal set, such that it will be a big benefit just to be able to quickly verify that a value isn't part of $S$. In that case we can use a **Bloom filter**.\n",
    "\n",
    "### Bloom Filter\n",
    "\n",
    "A Bloom filter is a probabilistic data structure that is used for testing whether a value is a member of a set. Instead of getting absolute *True* or *False* values when testing for membership, the Bloom filter instead returns True or False with the following meaning:\n",
    "\n",
    "1. True - Value is \"possibly\" in set.\n",
    "2. False - Value is **definitely** not in set.\n",
    "\n",
    "In other words the Bloom filter may return a false positive, but never a false negative. In return for this ambiguity the Bloom filter uses far less space than a hash table or a hash set, and is easier to keep in memory for fast lookups. There is a tradeoff between the space needed by the Bloom filter and the probability of getting a false positve, the larger the space utilized the less chance for getting false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb1c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup bloom filter and filtering functions\n",
    "\n",
    "size = 100000\n",
    "bloomFilter = np.zeros(size, dtype=int)\n",
    "\n",
    "#Filter for 10% of words\n",
    "filterSet = np.random.choice(words, int(len(words) / 10))\n",
    "\n",
    "\n",
    "for key in filterSet:\n",
    "    #Hash twice using different methods, then set the index for both to 1.\n",
    "    index1 = hashInt(key, buckets=size)\n",
    "    index2 = hashInt(key, buckets=size, hashType='md5')\n",
    "\n",
    "    bloomFilter[index1] = 1\n",
    "    bloomFilter[index2] = 1\n",
    "\n",
    "def inBloomFilter(val):\n",
    "    #Hash twice, then verify that all values of the hash indexes are 1\n",
    "    index1 = hashInt(val, buckets=size)\n",
    "    index2 = hashInt(val, buckets=size, hashType='md5')\n",
    "    \n",
    "    if bloomFilter[index1] == 1 and bloomFilter[index2] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02df6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 100000 Maybe in Filter Set: 21455 Really in Filter Set: 9572\n",
      "Iterations 200000 Maybe in Filter Set: 42892 Really in Filter Set: 19080\n",
      "Iterations 300000 Maybe in Filter Set: 64396 Really in Filter Set: 28566\n",
      "Iterations 400000 Maybe in Filter Set: 85674 Really in Filter Set: 37980\n",
      "Iterations 500000 Maybe in Filter Set: 106912 Really in Filter Set: 47438\n",
      "Iterations 600000 Maybe in Filter Set: 128308 Really in Filter Set: 56984\n",
      "Iterations 700000 Maybe in Filter Set: 149718 Really in Filter Set: 66486\n",
      "Iterations 800000 Maybe in Filter Set: 171132 Really in Filter Set: 76124\n",
      "Iterations 900000 Maybe in Filter Set: 192420 Really in Filter Set: 85549\n",
      "Iterations 1000000 Maybe in Filter Set: 213840 Really in Filter Set: 95106\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "bloomYes = 0\n",
    "actualYes = 0\n",
    "\n",
    "\n",
    "while n < 1000000:\n",
    "    word = np.random.choice(words)\n",
    "    \n",
    "    if inBloomFilter(word):\n",
    "        bloomYes += 1\n",
    "        \n",
    "        if word in filterSet:\n",
    "            actualYes += 1\n",
    "            \n",
    "    n += 1\n",
    "    if n % 100000 == 0:\n",
    "        print('Iterations', n, 'Maybe in Filter Set:', bloomYes, 'Really in Filter Set:', actualYes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cb74b",
   "metadata": {},
   "source": [
    "### Probability of False Positives\n",
    "\n",
    "Let $n$ be the length of the bit-array, $m$ be the size of set $S$, and $k$ be the number of hash functions we're using. The probability of getting a false positive is $(1-e^\\frac{-km}{n})^k$. For our example above, with $n=100000$, $m=23588$, and $k=2$, the probabity of a false positive is $14.14\\%$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160766ca",
   "metadata": {},
   "source": [
    "## Counting Distinct Elements in a Stream\n",
    "\n",
    "Another common problem is counting the number of distinct elements that we have seen in the stream so far. Again the assumption here is that the universal set of all elements is too large to keep in memory, so we'll need to find another way to count how many distinct values we've seen. If we're okay with simply getting an estimate instead of the actual value, we can use the Flajolet-Martin (FM) algorithm.\n",
    "\n",
    "### Flajolet-Martin Algorithm\n",
    "\n",
    "In the FM algorithm we hash the elements of a strea into a bit string. A bit string is a sequence of zeros and ones, such as 1011000111010100100. A bit string of length $N$ can hold $2^N$ possible combinations. For the FM algorithm to work, we need $N$ to be large enough such that the bit string will have more possible combinations than there are elements in the universal set. This basically means that there should be no possible collisions when we has the elements into a bit string. \n",
    "\n",
    "The idea behind the FM algorithm is that the more distinct elements we see, the higher the likelihood that one of their hash values will be \"unusual\". The specific \"unusualness\" we will exploit here is that the bit string ends in many consecutive 0s.\n",
    "\n",
    "For example, the bit string 1011000111010100100 ends with 2 consecutive zeros. We call this value of 2 the *tail length* of the bit string. Now let $R$ be the maximum tail length of that we have seen of any hashed bit string of the stream. The estimate of the number of distinct elements using FM is simply $2^R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d1ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 True Distinct: 81616 FM Approx: 65536\n",
      "200000 True Distinct: 134838 FM Approx: 524288\n",
      "300000 True Distinct: 169837 FM Approx: 524288\n",
      "400000 True Distinct: 192620 FM Approx: 524288\n",
      "500000 True Distinct: 207627 FM Approx: 524288\n",
      "600000 True Distinct: 217436 FM Approx: 524288\n",
      "700000 True Distinct: 223843 FM Approx: 524288\n",
      "800000 True Distinct: 228031 FM Approx: 524288\n",
      "900000 True Distinct: 230650 FM Approx: 524288\n",
      "1000000 True Distinct: 232459 FM Approx: 524288\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "trueDistinct = set()\n",
    "maxTailLength = -1\n",
    "\n",
    "while n < 1000000:\n",
    "    word = np.random.choice(words)\n",
    "    trueDistinct.add(word)\n",
    "    \n",
    "    bitString = hashBits(word)\n",
    "    tailLength = 0\n",
    "    for i in reversed(range(len(bitString))):\n",
    "        if bitString[i] == '0':\n",
    "            tailLength += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if tailLength > maxTailLength:\n",
    "        maxTailLength = tailLength\n",
    "        \n",
    "    n += 1\n",
    "    if n % 100000 == 0:\n",
    "        print(n, 'True Distinct:', len(trueDistinct), 'FM Approx:', 2**maxTailLength)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcaf16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536\n"
     ]
    }
   ],
   "source": [
    "#Library function for non-streams\n",
    "def flajoletMartin(array):\n",
    "    max_tail_length = 0\n",
    "    \n",
    "    for val in array:\n",
    "        bit_string = bin(hash(hashlib.md5(val.encode('utf-8')).hexdigest()))\n",
    "        \n",
    "        i = len(bit_string) - 1\n",
    "        tail_length = 0\n",
    "        while i >= 0:\n",
    "            if bit_string[i] == '0':\n",
    "                tail_length += 1\n",
    "            else:\n",
    "                #neatly handles the '0b' prefix of the binary string too. \n",
    "                #Just break when we see \"b\"\n",
    "                break\n",
    "                \n",
    "            i -= 1\n",
    "            \n",
    "        if tail_length > max_tail_length:\n",
    "            max_tail_length = tail_length\n",
    "            \n",
    "    return (2**max_tail_length)\n",
    "\n",
    "testList = []\n",
    "n=0\n",
    "while n < 100000:\n",
    "    n += 1\n",
    "    testList.append(np.random.choice(words))\n",
    "\n",
    "print(flajoletMartin(testList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22af7",
   "metadata": {},
   "source": [
    "### Using Multiple Hash Functions\n",
    "\n",
    "We can improve the estimate further by using multiple hash functions. With multiple hash functions we first split hash functions into groups, get the maximum tail length for each hash function, then get the average for each group. Lasly, we get the median over all the averages and that will be our estimate.\n",
    "\n",
    "This way we can get estimates that aren't just powers of 2. If the correct count is between two large powers of 2, for example 7000, it will be impossible to get a good estimate using just one hash function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460beb72",
   "metadata": {},
   "source": [
    "## Moments\n",
    "\n",
    "Computing the \"moments\" of a stream involves getting the distribution of the different elements in a stream, and is a generalization of the previous problem of counting distinct elements. Let $m_i$ be the number of occurences of element $i$ in a stream. The *kth-order moment* of a stream is the sum over all $i$ of $(m_i)^k$, or $\\sum_i(m_i)^k$.\n",
    "\n",
    "The 0th order moment of a stream is the sum of $(m_i)^0$, or the sum of 1 for each element $i$ found in the stream, or simply the count of distinct elements in the stream. We can use the Flajolet-Martin algorithm detailed above for this.\n",
    "\n",
    "The 1st moment is the sum of $m_i$'s, or simply the length of the stream.\n",
    "\n",
    "The 2nd moment is the sum $(m_i)^2$. This measures how uneven the distribution of the elements of the stream is, and is also called the *surprise number*. Again, if the we have the capacity to store $m_i$ for each element of the stream then this is easy to calculate. However if the number of elements if too large we need to look for alternatives such as approximation. One way to estimate the 2nd moment of the stream is to use the Alon-Matias-Szegedy (AMS) algorithm.\n",
    "\n",
    "### Alon-Matias-Szegedy Algorithm for Second Moments\n",
    "\n",
    "In the AMS algoritm we calculate a number of variables $X$ as we go through the stream. The more $X$'s we calculate, the more accurate our estimate will be. Each element $X$ contains:\n",
    "\n",
    "1. $X.element$ - an element of the universal set found in the stream.\n",
    "2. $X.value$ - the number of times we have seen $X.element$ in the stream since we have started tracking it.\n",
    "\n",
    "The estimate of the 2nd moment is $n \\times (2 \\times X.value âˆ’ 1)$, where $n$ is the length of the stream. We improve the final estimate by averaging the estimate of all $X$'s. One thing to note is we do not start tracking all the $X$ variables at the start of the stream. Instead, we start tracking each $X$ variable at random points as we go through the stream. \n",
    "\n",
    "As an example, suppose we have a stream of length $n=15$ and having values *a,b,c,b,d,a,c,d,a,b,d,c,a,a,b*. $m_a=5$, $m_b=4$, $m_c=3$, and $m_d=3$. The surprise number for this stream is therefore $52 + 42 + 32 + 32 = 59$. Let's calculate 3 variables $X_1$, $X_2$, and $X_3$, starting at 3rd, 8th, and 13th positions respectively at \"random\".\n",
    "\n",
    "* At position 3 we find *c*, so we set $X_1.element$ to *c* and $X_1.value$ to 1. We'll encounter two more *c* as we go through the stream so the final value of $X_1.value$ is 3.\n",
    "\n",
    "* At position 8 we find *d*, so $X_2.element=d$ and $X_2.value=1$. At the end of the stream $X_2.value=2, since we only count starting from position 8.\n",
    "\n",
    "* At position 13 is *a*, so $X_3.element=a$. The final value of $X_3.value$ is 2.\n",
    "\n",
    "We calculate the estimate as $n \\times (2 \\times X.value âˆ’ 1)$, so that's $15Ã—(2Ã—3âˆ’1)=75$ for $X_1$ and $15Ã—(2Ã—2âˆ’1)=45$ for both $X_2$ and $X_3$. The average of the estimates is $(75+45+45) \\div 3 = 55$, which is close to the actual value of $59$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f50407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100000 Real 2nd Moment: 142268 Approx 2nd Moment: 142076.22356186778\n",
      "Iteration 200000 Real 2nd Moment: 369446 Approx 2nd Moment: 369975.5602932765\n",
      "Iteration 300000 Real 2nd Moment: 680670 Approx 2nd Moment: 686277.8972605155\n",
      "Iteration 400000 Real 2nd Moment: 1077736 Approx 2nd Moment: 1097115.2541452956\n",
      "Iteration 500000 Real 2nd Moment: 1559600 Approx 2nd Moment: 1598531.5647043167\n",
      "Iteration 600000 Real 2nd Moment: 2127440 Approx 2nd Moment: 2195491.3622203344\n",
      "Iteration 700000 Real 2nd Moment: 2778316 Approx 2nd Moment: 2883885.3955207868\n",
      "Iteration 800000 Real 2nd Moment: 3513340 Approx 2nd Moment: 3668286.941681869\n",
      "Iteration 900000 Real 2nd Moment: 4334438 Approx 2nd Moment: 4545672.967889112\n",
      "Iteration 1000000 Real 2nd Moment: 5239120 Approx 2nd Moment: 5536791.019167607\n"
     ]
    }
   ],
   "source": [
    "variables = {}\n",
    "realDistribution = {}\n",
    "\n",
    "n = 0\n",
    "\n",
    "while n < 1000000:\n",
    "    word = np.random.choice(words)\n",
    "    \n",
    "    if word in variables:\n",
    "        variables[word] += 1\n",
    "    else:\n",
    "        if random.random() < 0.1: #add to variables with 10% chance\n",
    "            variables[word] = 1\n",
    "    \n",
    "    #Get the real distribution for validation\n",
    "    if word not in realDistribution:\n",
    "        realDistribution[word] = 0\n",
    "        \n",
    "    realDistribution[word] += 1\n",
    "    \n",
    "    n += 1\n",
    "    if n % 100000 == 0:\n",
    "        real2ndMoment = 0\n",
    "        for w in realDistribution:\n",
    "            real2ndMoment += realDistribution[w] ** 2\n",
    "    \n",
    "        #Calculate 2nd moment using the AMS variables\n",
    "        approx2ndMoment = 0\n",
    "        for w in variables:\n",
    "            approx2ndMoment += n * ((2 * variables[w]) - 1)\n",
    "            \n",
    "        approx2ndMoment /= len(variables)\n",
    "\n",
    "        print('Iteration', n, 'Real 2nd Moment:', real2ndMoment, 'Approx 2nd Moment:', approx2ndMoment)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8db92",
   "metadata": {},
   "source": [
    "### Higher Order Elements\n",
    "\n",
    "We can estimate higher-order elements similar to how we estimate 2nd-order elements. The only difference is instead of $n \\times (2 \\times X.value âˆ’ 1)$ for the estimate, the formula is now $n \\times (X.value^kâˆ’(X.valueâˆ’1)^k)$.\n",
    "\n",
    "Below we provide an alonMatiasSzegedy() function that takes as argument the moment value to approximate for, the number of variables to track, and the probability of an element in an array becoming a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "163c740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233333.33333333334\n"
     ]
    }
   ],
   "source": [
    "def alonMatiasSzegedy(array, k_moment=2, num_variables=3, var_prob=0.1):\n",
    "    variables = {}\n",
    "    n = len(array)\n",
    "  \n",
    "    for val in array:\n",
    "        if val not in variables:\n",
    "            if len(variables) < num_variables and random.random() < var_prob:\n",
    "                variables[val] = 1\n",
    "        else:\n",
    "            variables[val] += 1\n",
    "\n",
    "\n",
    "    amsMoment = 0\n",
    "    for w in variables:\n",
    "        amsMoment +=   n * (variables[w]**k_moment - ((variables[w] - 1)**k_moment))\n",
    "\n",
    "    amsMoment /= len(variables)\n",
    "    return amsMoment\n",
    "\n",
    "testList = []\n",
    "n=0\n",
    "while n < 100000:\n",
    "    n += 1\n",
    "    testList.append(np.random.choice(words))\n",
    "    \n",
    "print(alonMatiasSzegedy(testList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb4c83",
   "metadata": {},
   "source": [
    "## Counting Ones in a Window\n",
    "\n",
    "Suppose we have a bit stream window of length $N$ and we want to count the number of ones in the last $k$ bits. Again assume that $k$ is too large so we cannot afford to store it all $k$ bits in memory, as would be needed to get an exact count. We can use the Datar-Gionis-Indyk-Motwani (DGIM) algorithm to approximate the count with an error rate of no greater than 50%.\n",
    "\n",
    "### Datar-Gionis-Indyk-Motwani Algorithm\n",
    "\n",
    "Assume we have a bit stream window of '10101100111011011000101110110010110'. We give each bit a timestamp starting with 1 for the earliest bit, 2 for the 2nd bit, and so on. We divide the bits into *buckets* consisting of:\n",
    "\n",
    "1. The timestamp of its most rightmost (most recent) end.\n",
    "2. The number of ones in the bucket. This number must be a power of 2, and we refer to this as the *size* of the bucket.\n",
    "\n",
    "Furthermore, the buckets of the bit stream have to follow the following rules:\n",
    "\n",
    "1. The right end of a bucket is always a bit with a value 1.\n",
    "2. Every bit with a value of 1 is in some bucket.\n",
    "3. No bit is in more than one bucket.\n",
    "4. There are just one or two buckets of any given size.\n",
    "5. All sizes must be a power of 2.\n",
    "6. Buckets cannot decrease in size as we move to the left.\n",
    "\n",
    "Once we've split the bit stream into buckets, we can now estimate the number of 1's for any $k$ in the window. Find the bucket $b$ with the earliest timestamp that includes some of the $k$ most recent bits. The estimate of the number of ones is the sum of the sizes of all buckets to the right of $b$, plus half the size of $b$ itself.\n",
    "\n",
    "For example, take the stream above having the buckets as shown and with $k=10$. We want to get the count of ones in the 10 rightmost bits, or 0110010110. The bucket with size 4 containing '11101' is the earliest bucket which contains the at least some of last $k$ bits. We take half the size of this bucket, which equals to 2, plus the sizes of the buckets to the right, so 2, 1, and 1. The DGIM estimate for the number of ones is therefore $2+2+1+1=6$, which is close to the actual answer of 5.\n",
    "\n",
    "Let's try another example, this time counting the ones in the last **20** elements of bit stream ...100011101011010001110111101000001010101001000000000100101001100, divided into the following buckets:\n",
    "\n",
    "<img src='BitBuckets2.png' heigh=\"250\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1859e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitStream = '100011101011010001110111101000001010101001000000000100101001100'\n",
    "\n",
    "k=20 #Count ones in last k=20 elements\n",
    "buckets = {}\n",
    "buckets[2] = 1\n",
    "buckets[3] = 1\n",
    "buckets[6] = 2\n",
    "buckets[11] = 2\n",
    "buckets[24] = 4\n",
    "buckets[36] = 4\n",
    "buckets[41] = 8\n",
    "buckets[56] = 8\n",
    "\n",
    "earliestBucket = True\n",
    "approxCount = 0\n",
    "for ts in sorted(buckets.keys(), reverse=True):\n",
    "    if ts > k:\n",
    "        continue\n",
    "        \n",
    "    if earliestBucket:\n",
    "        approxCount += int(buckets[ts] / 2)\n",
    "        earliestBucket = False\n",
    "    else:\n",
    "        approxCount += buckets[ts]\n",
    "        \n",
    "print('Approx Count', approxCount)\n",
    "\n",
    "realCount = 0\n",
    "\n",
    "for b in range(k):\n",
    "    if bitStream[-b-1] == '1':\n",
    "        realCount += 1\n",
    "\n",
    "print('Real Count', realCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4f828",
   "metadata": {},
   "source": [
    "### Updating Buckets with New Data\n",
    "\n",
    "When a new data comes in from the stream, we may need to update the buckets in a way that everything will still follow the DGIM conditions. When a new bit enters, we take the following steps:\n",
    "\n",
    "1. Check the leftmost bucket. If the timestamp has has gone past the our window of length $N$,remove it from the list of buckets we are tracking.\n",
    "\n",
    "2. If the new bit is 0, we do not need to do anything. \n",
    "\n",
    "3. If the new bit is 1, we first create a new bucket of size 1 containing the new bit. If there was only one bucket of size 1, then nothing more needs to be done.\n",
    "\n",
    "4. If there are now 3 buckets of size 1, we combine the 2 leftmost buckets of size into one bucket of size 2.\n",
    "\n",
    "5. Do step #4 repeatedly for the buckets with sizes greater than 1 until all the buckets follow the DGIM rules once again.\n",
    "\n",
    "<img src=\"BitBuckets2Add.png\" heigh=\"250\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7171d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{57: 16, 42: 8, 37: 8, 25: 4, 12: 4, 7: 2, 4: 2, 3: 1, 0: 1}\n"
     ]
    }
   ],
   "source": [
    "N = 60\n",
    "\n",
    "#Existing bucket groupings\n",
    "buckets = {}\n",
    "buckets[2] = 1\n",
    "buckets[3] = 1\n",
    "buckets[6] = 2\n",
    "buckets[11] = 2\n",
    "buckets[24] = 4\n",
    "buckets[36] = 4\n",
    "buckets[41] = 8\n",
    "buckets[56] = 8\n",
    "\n",
    "\n",
    "for key in sorted(buckets.keys(), reverse=True): #add all existing buckets back with their rightmost index incremented by 1\n",
    "    if key+1 > N: #No need to add if bucket is now out of window\n",
    "        continue\n",
    "        \n",
    "    buckets[key+1] = buckets[key]\n",
    "    del buckets[key]\n",
    "\n",
    "buckets[0] = 1 #create a new bucket of size 1 containing the new bit\n",
    "\n",
    "for size in [1,2,4,8,16]:\n",
    "    buckets_ = [k for k,v in buckets.items() if v==size]\n",
    "    if len(buckets_) > 2:\n",
    "        max_bucket = max(buckets_)\n",
    "        buckets[max_bucket] = size*2\n",
    "        \n",
    "    continue\n",
    "    \n",
    "    sizeCount = 0\n",
    "    sizeIndexes = []\n",
    "    \n",
    "    for key in sorted(buckets.keys()):\n",
    "        if buckets[key] == size:\n",
    "            sizeCount += 1\n",
    "            sizeIndexes.append(key)\n",
    "\n",
    "        elif buckets[key] > size:\n",
    "            break\n",
    "    \n",
    "    #Only need to do something if more than 2 buckets have the same size\n",
    "    if sizeCount > 2:\n",
    "        newSize = size * 2\n",
    "        \n",
    "        #combine the 2 leftmost buckets of the same size into one bucket of twice the size.\n",
    "        buckets[sizeIndexes[1]] = newSize\n",
    "        del buckets[sizeIndexes[2]]\n",
    "\n",
    "    \n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc33ec1",
   "metadata": {},
   "source": [
    "### Error Bounds\n",
    "\n",
    "Suppose that the exact count is $c$, and the DGIM estimate involves a leftmost bucket $b$ of size $2^j$. We consider 2 error cases separately, one where the estimate is larger than $c$ and one where the estimate is smaller than $c$.\n",
    "\n",
    "1. The estimate is smaller than $c$. The worst case scenario is all the 1's in $b$ should have been part of the count, and using only $\\frac{1}{2}$ of the size of $b$ only gives us $2^{j-1}$ 1's. However $c$ should be at least $2^{j+1}-1$. Thereforce we conclude that our estimate is at least 50% of $c$.\n",
    "\n",
    "2. The estimate is larger than $c$. The worst case scenario is only the rightmost bit of bucket $b$ is included, and there is only one bucket of each of the sizes smaller than $b$. Then $c = 1+2^{jâˆ’1} +2^{jâˆ’2} +Â·Â·Â·+1=2^j$, and the estimate we get from DGIM is $2^{jâˆ’1} + 2^{jâˆ’1} + 2^{jâˆ’2} + Â· Â· Â· + 1 = 2^j + 2^{jâˆ’1} âˆ’ 1$. Therefor we can conclude that the estimate is no more than 50% greater of $c$. \n",
    "\n",
    "We can improve the error bounds further by replacing the condition that there can only be 1 or 2 buckets for each size, with instead there being $r$ or $r-1$ buckets for each size with $r > 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32513fe",
   "metadata": {},
   "source": [
    "## Most Common Recent Elements\n",
    "\n",
    "\n",
    "Previous counting problems have assumed a sliding window that holds the tail-end of a stream for processing. However sometimes we want to consider all the elements in the stream from the start while also distinguising between really old elements and data that is more recent. For example, we want to find out what movies are popular currently. While \"currently\" is a relatively ambiguous term, we do know that we want to discount movies that were popular decades ago, like *The Godfather*, and emphasize more recent movies like *Avengers: Endgame*. Imagine that we have a stream of movies being watched coming in according to increasing timestamp. A simple way to get the get most common movies would be to calculate the count for each movie, possibly even with some of the techniques above (i.e., counting ones). However, this will only give us an approximation and won't satisfy our preference to discount older movies. One way to do this is through the technique of Decaying Windows.\n",
    "\n",
    "### Decaying Windows\n",
    "\n",
    "In decaying windows, we exponentially decay the count for each movie by a constant $c$. $c$ will have a really small value, like $10^{-6}$ or $10^{-9}$. The value for each element is decayed by multiplying the current count by $(1-c)$ at every time step. Given a stream $a_1, a_2,...,a_t$, the exponentially decaying window of this stream is \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=0}^{t-1}a_{t-i}(1-c)^i\n",
    "\\end{equation}\n",
    "\n",
    "When a new element $a_{t+1}$ is added into the stream all we need to do is:\n",
    "\n",
    "1. Multiply the current sum by $1-c$.\n",
    "2. Add $a_{t+1}$.\n",
    "\n",
    "In our movie example, we imagine a separate stream for each movie with a 1 each time that movie is watched, and a 0 each time another movie is watched. The decaying sum of the 1â€™s measures the current popularity of the movie. To save space, we can completely disregard unpopular movies by dropping movies whose count falls below a certain threshold, such as 0.5. When new data arrives at the stream we do the following:\n",
    "\n",
    "1. For each movie whose score we are currently tracking, multiply the score by $(1-c)$.\n",
    "2. If the data is for movie $M$, add 1 to $M$'s score if we are currently tracking it, or start tracking $M$ and initialize it with a score of 1 if we are not.\n",
    "3. Drop any movies we are tracking if the score is less than our threshold of 0.5.\n",
    "\n",
    "Due to the exponential decay, the sum of all scores is $\\frac{1}{c}$. There cannot be more than $\\frac{2}{c}$ movies with score of 0.5 or more, otherwise the sum of scores would be greater than $\\frac{1}{c}$. Therefore $\\frac{2}{c}$ is the maximum number of scores we would have to track.\n",
    "\n",
    "Let's provide an example using the MovieLens 100k dataset, a list of movie ratings from September 20, 1997 to April 22, 1998. First, let's map movies to their release timestamp so we can easily sort them by when they were released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06f206df",
   "metadata": {},
   "outputs": [],
   "source": [
    "movieStream = {}\n",
    "movieNames = {}\n",
    "\n",
    "\n",
    "for line in open('ml-100k/u.item', encoding = \"ISO-8859-1\"):\n",
    "    tokens = line.strip().split('|')\n",
    "    movieNames[tokens[0]] = tokens[1]\n",
    "    #print(tokens[0], tokens[1])\n",
    "    \n",
    "for line in open('ml-100k/u.data'):\n",
    "    tokens = line.strip().split()\n",
    "    movie = tokens[1]\n",
    "    ts = tokens[3]\n",
    "    \n",
    "    #handle duplicate timestamps\n",
    "    while ts in movieStream:\n",
    "        ts += 'a'\n",
    "    \n",
    "    movieStream[ts] = movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b5f76",
   "metadata": {},
   "source": [
    "Next we count movie rating with decaying windows. Play with the variable $c$ below by setting it to 0 to see which movies would be most popular by simple raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0390878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997) 67.28333046382443\n",
      "Contact (1997) 55.42960303711523\n",
      "Air Force One (1997) 54.49994366275277\n",
      "Star Wars (1977) 50.654564738716964\n",
      "Full Monty, The (1997) 49.25542831929427\n",
      "English Patient, The (1996) 46.814044430751856\n",
      "Good Will Hunting (1997) 45.61329962426032\n",
      "Liar Liar (1997) 44.01656947009622\n",
      "Scream (1996) 43.58306822429063\n",
      "Fargo (1996) 40.69582727523209\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "c = 0.0001\n",
    "\n",
    "n = 0\n",
    "\n",
    "for key in sorted(movieStream.keys()):\n",
    "    movie = movieStream[key]\n",
    "    \n",
    "    remove = set()\n",
    "    \n",
    "    for key in scores:\n",
    "        scores[key] *= (1-c)\n",
    "        if scores[key] < 0.5 and key != movie:\n",
    "            remove.add(key)\n",
    "\n",
    "    \n",
    "    for rem in remove:\n",
    "        del scores[rem]\n",
    "    \n",
    "    if movie in scores:\n",
    "        scores[movie] += 1\n",
    "    else:\n",
    "        scores[movie] = 1\n",
    "        \n",
    "    n += 1\n",
    "    if n == 100000:\n",
    "        break\n",
    "\n",
    "sortedScores = {k: v for k, v in sorted(scores.items(), reverse=True, key=lambda item: item[1])}\n",
    "count = 0\n",
    "for movie in sortedScores:\n",
    "    print(movieNames[movie], sortedScores[movie])\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd503c2f",
   "metadata": {},
   "source": [
    "We find that Titanic is the most popular movie by our method, even though by raw count it wouldn't even hit the top 10. This is because Titanic was released late 1997 so the reviews for it would be more recent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049011f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Jure Leskovec, Anand Rajaraman, and Jeffrey D Ullman. Mining of massive datasets, chapter 4. Cambridge University Press, 3rd edition, 2019. http://www.mmds.org/.\n",
    "\n",
    "* Bloom, Burton H. \"Space/ time trade-offs in hash coding with allowable errors .\" Communications of the ACM 13.7 (1970): 422-426.\n",
    "\n",
    "* Apache Kafka https://kafka.apache.org/\n",
    "\n",
    "* MovieLens 100K http://grouplens.org/datasets/movielens/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
