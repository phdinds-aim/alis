{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae95b85",
   "metadata": {},
   "source": [
    "# Stream Data Model\n",
    "\n",
    "```{figure} ./images/StreamDataModel.png\n",
    ":name: stream-data-model\n",
    ":width: 400px\n",
    "\n",
    "A canonical example of a streaming data management system.\n",
    "```\n",
    "\n",
    "We show a canonical example of a streaming data management system above (See {numref}`stream-data-model`). A number of stream is feeeing into the system, each providing data on its own schedule. The data from the stream need be uniform both in the data type and the rate at which the data arrives. The fact that the rate of arrival of stream elements is not under the control of the system is the main distinguishing factor between stream mining and a regular data mining system.\n",
    "\n",
    "Streams may eventually be saved in a longer term archival storage like a disk or database, but we have to assume that we cannot answer analytics queries using the archived data because retrieving it is a time-consuming process and the latency of the streaming data coming in is much faster. Instead there is a limited working storage which can be accessed faster that we can use, but it has insufficient capacity to store the entire stream.\n",
    "\n",
    "## Stream Sources\n",
    "\n",
    "1. Sensor Data\n",
    "2. Video Cameras\n",
    "3. Internet Web Traffic\n",
    "4. Server Logs\n",
    "\n",
    "## Stream Queries\n",
    "\n",
    "There are two main types of queries we want to make of streaming data. The first are standing queries which are permanently executing. For example if we have streaming data from temperature sensors, our standing queries could be:\n",
    "\n",
    "* Output alerts when temperature data passes a certain threshold.\n",
    "* Average temperature using the last $n$ elements in a stream.\n",
    "* What's the maximum temperature ever recorded in the stream\n",
    "\n",
    "All of these are calculations that can be done quickly when new data appears in the stream with no store the stream in its entirety. \n",
    "\n",
    "The other form of queries are *adhoc queries*. Here the queries can be one-time or the parameter of the query can be changing based on the need. A common approach would be to store a sliding window of the $n$ most recent data in the working storage of the stream management system. \n",
    "\n",
    "## Examples of Stream Processing Systems\n",
    "\n",
    "<img src=\"./images/kafka.png\" width=200>\n",
    "<img src=\"./images/kinesis.png\" width=200>\n",
    "\n",
    "Two examples of streaming systems are: Apache Kafka (https://kafka.apache.org) and Amazon Kinesis (https://aws.amazon.com/kinesis/). For the examples in this notebook we will Apache Kafka to simulate data coming from a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7afaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n",
    "import math\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "random.seed(10) #Set seed for repeatability\n",
    "\n",
    "#Hash a string to an integer\n",
    "def hashInt(val, buckets=sys.maxsize, hashType='default'):\n",
    "    if hashType == 'default':\n",
    "        return hash(val) % buckets\n",
    "    elif hashType == 'md5':\n",
    "        return hash(hashlib.md5(val.encode('utf-8')).hexdigest()) % buckets\n",
    "\n",
    "#Hash a string to a bit string\n",
    "def hashBits(val, hashType='default'):\n",
    "    if hashType == 'default':\n",
    "        return bin(hash(val))\n",
    "    elif hashType == 'md5':\n",
    "        return bin(hash(hashlib.md5(val.encode('utf-8')).hexdigest()))\n",
    "    \n",
    "words = np.array(open('words.txt').read().splitlines())\n",
    "userIds = np.arange(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e115e",
   "metadata": {},
   "source": [
    "## Kafka Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14549474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafka\n",
    "\n",
    "#Build our streams\n",
    "words = np.array(open('words.txt').read().splitlines()) #copied from /usr/share/dict/words\n",
    "userIds = np.arange(1, 1000)\n",
    "\n",
    "producer = kafka.KafkaProducer()\n",
    "\n",
    "#Create streams of 2 million elements\n",
    "for i in range(1000000):\n",
    "    if (i+1) % 100000 == 0:\n",
    "        print(i+1)\n",
    "        \n",
    "    producer.send('sampling', (str(np.random.choice(words))).encode('utf-8'))\n",
    "\n",
    "    producer.flush()\n",
    "    \n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8eb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = kafka.KafkaConsumer('test', auto_offset_reset='earliest')\n",
    "n = 0\n",
    "for msg in consumer:\n",
    "    print(msg.value.decode(\"utf-8\"))\n",
    "    n += 1\n",
    "    if n == 8:\n",
    "        break\n",
    "        \n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
